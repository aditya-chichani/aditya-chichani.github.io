| **Model**                            | **Prediction Function**                 | **Loss Function**                   | **Evaluation Function**     | **Use Case**                             | **Tradeoffs**                            | **Pros**                                 | **Cons**                                         | **Drawbacks**                                   | **Assumptions about Data**                   | **Prerequisites**                                            | **Feature Engineering**                           | **Hyperparameter Tuning**                         | **Overfitting Measures**         |
| ------------------------------------ | --------------------------------------- | ----------------------------------- | --------------------------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- | ------------------------------------------------ | ----------------------------------------------- | -------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------- | -------------------------------- |
| **Linear Regression**                | \(y = \beta_0 + \beta_1 x + \epsilon\)  | Mean Squared Error (MSE)            | R², Adjusted R²             | Regression                               | Simple, interpretable                    | Fast to train, easy to interpret         | Assumes linearity                                | Residuals are normally distributed              | Normality of residuals, no multicollinearity | Standardization can help                                     | Grid search, random search, Bayesian optimization | Regularization (L1, L2), Cross-validation         |
| **Logistic Regression**              | \(P(y=1\| X) = \frac{1}{1 + e^{-z}}\)       | Binary Cross-Entropy        | Accuracy, AUC-ROC                        | Binary Classification                    | Simple, interpretable                    | Fast to train, good for binary outcomes          | Assumes linear relationship with log-odds       | Independent observations                     | No multicollinearity                                         | One-hot encoding for categorical variables        | Grid search, random search                        | Regularization, Cross-validation |
| **Decision Trees**                   | Split based on feature thresholds       | Gini impurity, Entropy              | Accuracy, F1-score          | Classification, Regression               | Prone to overfitting                     | Easy to interpret and visualize          | Sensitive to noisy data                          | No assumptions about data distribution          | None required                                | None required                                                | Grid search, pruning, random search               | Pruning, Cross-validation, Ensemble methods       |
| **Random Forest**                    | Average predictions from multiple trees | Mean Squared Error (for regression) | Accuracy, OOB Error         | Classification, Regression               | More complex, slower                     | Reduces overfitting                      | Can be less interpretable                        | Assumes independence of features                | None required                                | Categorical to numerical conversion, handling missing values | Grid search, random search                        | OOB Error, Feature importance analysis            |
| **Support Vector Machine (SVM)**     | Decision boundary maximizing margin     | Hinge Loss                          | Accuracy, Precision, Recall | Binary Classification, Outlier Detection | Sensitive to parameter settings          | Effective in high-dimensional spaces     | Requires careful tuning of parameters            | Assumes linear separability (with kernel trick) | Feature scaling required                     | None required                                                | Grid search, random search                        | Cross-validation, regularization                  |
| **K-Nearest Neighbors (KNN)**        | Average/majority vote of neighbors      | None (distance metric)              | Accuracy, F1-score          | Classification, Regression               | Memory intensive, slow on large datasets | Simple to implement                      | Sensitive to noisy data, curse of dimensionality | No assumptions about data distribution          | None required                                | Standardization or normalization recommended                 | None                                              | Cross-validation                                  |
| **Neural Networks**                  | \(y = f(WX + b)\)                       | Mean Squared Error (for regression) | Accuracy, AUC-ROC           | Classification, Regression               | Requires large datasets, complex tuning  | Can model complex relationships          | Prone to overfitting                             | No assumptions about linearity                  | Proper preprocessing needed                  | Standardization, normalization, categorical encoding         | Grid search, random search, Bayesian optimization | Regularization (dropout, L1/L2), Cross-validation |
| **Gradient Boosting Machines (GBM)** | Ensemble of weak learners               | Mean Squared Error (for regression) | AUC-ROC, Log Loss           | Classification, Regression               | Slower to train, sensitive to noise      | Handles various data types well          | Can be less interpretable                        | Assumes independence of features                | Proper preprocessing needed                  | None required                                                | Grid search, random search                        | Early stopping, regularization                    |
| **XGBoost**                          | Ensemble of decision trees              | Log Loss, MSE                       | AUC-ROC, Log Loss           | Classification, Regression               | Complexity, requires tuning              | High performance, handles missing values | Can be less interpretable                        | Assumes independence of features                | Proper preprocessing needed                  | None required                                                | Grid search, random search                        | Early stopping, cross-validation                  |
