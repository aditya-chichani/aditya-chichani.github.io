---
layout: page
title: ""
---
# Call for Papers

This workshop aims to unite the research community to address multimodal challenges specifically
in the search and recommendation domain of IR. Given the recent advancements in multimodal LLMs that can democratize multimodal IR, we will use this workshop as a dedicated platform to discuss the latest research and challenges in this field.

### Theme : Transforming Search and Recommendations with Multimodal Approaches

#### Topics

Topics of interest include, but are not limited to:

1. **Cross-modal retrieval techniques**
   1. Strategies for efficiently indexing and retrieving multimodal data.
   2. Approaches to ensure cross-modal retrieval systems can handle large-scale data.
   3. Development of metrics to measure similarity across different data modalities.
2. **Applications of Multimodal Search and Recommendations to Verticals** (e.g. E-commerce,
real estate)
   1. Implementing and optimizing image-based product searches.
   2. Creating multimodal conversational systems to enhance user experience and make search more accessible.
   3. Utilizing AR to enhance product discovery and user interaction.
   4. Leveraging multimodal search for efficient customer service and support.
3. **User-centric design principles for multimodal search interfaces**
   1. Best practices for designing user-friendly interfaces that support multimodal search.
   2. Methods for evaluating the usability of multimodal search interfaces.
   3. Personalizing multimodal search interfaces to individual user preferences.
   4. Ensuring multimodal search interfaces are accessible to users with disabilities.
4. **Ethical Considerations and Privacy Implications of Multimodal Search and Recommendations**
   1. Strategies for ensuring user data privacy in multimodal applications.
   2. Identifying and mitigating biases in multimodal algorithms.
   3. Ensuring transparency in how multimodal results are generated and presented.
   4. Approaches for obtaining and managing user consent for using their data.
5. **Modeling for Multimodal Search and Discovery**
   1. Multi-modal representation learning
   2. Utilizing GPT-4o, Gemini, and other advanced pre-trained multimodal LLMs
   3. Dimensionality reduction techniques to reduce complexity of multimodal data.
   4. Techniques for fine-tuning pre-trained vision-language models.
   5. Developing and standardizing metrics to evaluate the performance of vision-language models in multimodal search.


### Submission Instructions

All papers will be peer reviewed by the program committee and judged by their relevance to the workshop and their potential to generate discussion. 

All submissions must be in PDF formatted according to the latest [CEUR single column format](https://www.overleaf.com/latex/templates/template-for-submissions-to-ceur-workshop-proceedings-ceur-ws-dot-org/wqyfdgftmcfw). The short (8-page) and long (15-page) limits are extended to account for this.

For instructions and LaTeX/Overleaf/docx templates, see [here](https://ceur-ws.org/HOWTOSUBMIT.html#CEURART). Read up to and including the “License footnote in paper PDFs” section. Please Use Emphasizing Capitalized Style for Paper Titles.

Submissions must describe work that is not previously published, not accepted for publication elsewhere, and not currently under review elsewhere. All submissions must be in English. The workshop follows a **single-blind** reviewing process. We do not accept anonymized submissions. Please note that at least one of the authors of each accepted paper **must** register for the workshop and present the paper.

- Long paper limit: 15 pages. 
- Short paper limit: 8 pages.
{: .box-note}
References are not counted in the page limit.

The link for submissions to CIKM MMSR'24 will be posted here

The deadline for paper submission is **July 29, 2024** (11: 59 P.M. AoE)